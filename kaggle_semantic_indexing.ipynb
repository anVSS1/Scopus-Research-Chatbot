{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de004b6f",
   "metadata": {},
   "source": [
    "# üî¨ Enhanced Semantic Indexing for Scopus Research Chatbot\n",
    "\n",
    "## üìä Multi-Index FAISS System for Scientific Literature Search\n",
    "\n",
    "This notebook creates **multiple specialized FAISS indexes** for different search strategies on 4,000+ scientific articles from Scopus (2018-2025). The enhanced system supports:\n",
    "\n",
    "- **Content Index**: Title + Abstract (primary semantic search)\n",
    "- **Metadata Index**: Content + Keywords + Authors  \n",
    "- **Institution Index**: Institution names + Countries\n",
    "- **Full Index**: All available text fields combined\n",
    "\n",
    "### üéØ Key Features:\n",
    "- **SPECTER embeddings** optimized for scientific papers\n",
    "- **Multi-modal search** combining semantic and metadata filtering\n",
    "- **Production-ready** for Hugging Face Spaces deployment\n",
    "- **4,254+ articles** across 15 scientific domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a828751",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "Install the necessary packages for semantic indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c182b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers faiss-cpu transformers torch\n",
    "\n",
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # If GPU is available, install faiss-gpu for better performance\n",
    "    %pip install faiss-gpu\n",
    "\n",
    "# Enhanced Semantic Indexing System\n",
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_NAME = 'scopus_database.db'\n",
    "\n",
    "# Multiple FAISS indexes for different search types\n",
    "INDEXES = {\n",
    "    'content': {\n",
    "        'faiss_file': 'scopus_content_index.faiss',\n",
    "        'ids_file': 'scopus_content_ids.json',\n",
    "        'description': 'Title + Abstract (primary semantic search)'\n",
    "    },\n",
    "    'metadata': {\n",
    "        'faiss_file': 'scopus_metadata_index.faiss', \n",
    "        'ids_file': 'scopus_metadata_ids.json',\n",
    "        'description': 'Title + Abstract + Keywords + Authors'\n",
    "    },\n",
    "    'institution': {\n",
    "        'faiss_file': 'scopus_institution_index.faiss',\n",
    "        'ids_file': 'scopus_institution_ids.json', \n",
    "        'description': 'Institution names and countries'\n",
    "    },\n",
    "    'full': {\n",
    "        'faiss_file': 'scopus_full_index.faiss',\n",
    "        'ids_file': 'scopus_full_ids.json',\n",
    "        'description': 'All available text fields combined'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Enhanced semantic indexing system loaded\")\n",
    "print(f\"üìä Will create {len(INDEXES)} specialized FAISS indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9fbbc",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = '/kaggle/input'  # Kaggle input directory\n",
    "OUTPUT_DIR = '/kaggle/working'  # Kaggle output directory\n",
    "\n",
    "# Find the database file in input directory\n",
    "db_files = list(Path(INPUT_DIR).rglob('*.db'))\n",
    "if db_files:\n",
    "    DATABASE_PATH = str(db_files[0])\n",
    "    print(f\"Found database: {DATABASE_PATH}\")\n",
    "else:\n",
    "    print(\"‚ùå No .db file found in input. Please upload your scopus_database.db file.\")\n",
    "    DATABASE_PATH = None\n",
    "\n",
    "# Output files\n",
    "FAISS_INDEX_FILE = os.path.join(OUTPUT_DIR, \"scopus_combined_metadata_index.faiss\")\n",
    "ARTICLE_IDS_MAP_FILE = os.path.join(OUTPUT_DIR, \"scopus_article_ids_for_index.json\")\n",
    "\n",
    "print(f\"Output files will be saved to:\")\n",
    "print(f\"- FAISS Index: {FAISS_INDEX_FILE}\")\n",
    "print(f\"- Article IDs: {ARTICLE_IDS_MAP_FILE}\")\n",
    "\n",
    "def get_article_data_with_affiliations():\n",
    "    \"\"\"Get articles with their affiliation information for enhanced indexing.\"\"\"\n",
    "    print(\"üìä Connecting to database and fetching article data...\")\n",
    "    \n",
    "    conn = sqlite3.connect(DATABASE_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Extended query to include affiliations and countries for enhanced search\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            A.scopus_id, \n",
    "            A.title, \n",
    "            A.abstract, \n",
    "            A.cover_date, \n",
    "            A.keywords,\n",
    "            GROUP_CONCAT(Auth.full_name, '; ') AS authors_list,\n",
    "            GROUP_CONCAT(Aff.institution_name, '; ') AS affiliations_list,\n",
    "            GROUP_CONCAT(Aff.country, '; ') AS countries_list\n",
    "        FROM articles AS A\n",
    "        LEFT JOIN article_authors AS AA ON A.scopus_id = AA.article_scopus_id\n",
    "        LEFT JOIN authors AS Auth ON AA.author_id = Auth.author_id\n",
    "        LEFT JOIN author_affiliations AS AuthAff ON Auth.author_id = AuthAff.author_id\n",
    "        LEFT JOIN affiliations AS Aff ON AuthAff.affiliation_id = Aff.affiliation_id\n",
    "        WHERE A.abstract IS NOT NULL AND A.abstract != '' \n",
    "        GROUP BY A.scopus_id, A.title, A.abstract, A.cover_date, A.keywords\n",
    "        ORDER BY A.scopus_id\n",
    "    ''')\n",
    "    \n",
    "    articles_data = cursor.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"‚úÖ Retrieved {len(articles_data):,} articles with complete metadata\")\n",
    "    return articles_data\n",
    "\n",
    "# Load the data\n",
    "articles_data = get_article_data_with_affiliations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf2b37",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATABASE_PATH = 'path_to_your_database.db'  # Update this path\n",
    "\n",
    "def create_embeddings_for_index_type(articles_data, index_type):\n",
    "    \"\"\"Create embeddings based on index type for specialized search.\"\"\"\n",
    "    print(f\"üîç Creating embeddings for {index_type} index...\")\n",
    "    \n",
    "    texts_to_embed = []\n",
    "    article_ids = []\n",
    "    \n",
    "    for row in tqdm(articles_data, desc=f\"Processing {index_type}\"):\n",
    "        # Handle the enhanced data structure: scopus_id, title, abstract, cover_date, keywords, authors_list, affiliations_list, countries_list\n",
    "        scopus_id, title, abstract, cover_date, keywords, authors_list, affiliations_list, countries_list = row\n",
    "        \n",
    "        # Build text based on index type\n",
    "        if index_type == 'content':\n",
    "            # Primary content search (title + abstract only)\n",
    "            text = \"\"\n",
    "            if title:\n",
    "                text += f\"{title}. \"\n",
    "            if abstract:\n",
    "                text += f\"{abstract}\"\n",
    "                \n",
    "        elif index_type == 'metadata':\n",
    "            # Content + metadata\n",
    "            text = \"\"\n",
    "            if title:\n",
    "                text += f\"{title}. \"\n",
    "            if abstract:\n",
    "                text += f\"{abstract}. \"\n",
    "            if keywords:\n",
    "                text += f\"Keywords: {keywords}. \"\n",
    "            if authors_list and authors_list != 'None':\n",
    "                text += f\"Authors: {authors_list}. \"\n",
    "                \n",
    "        elif index_type == 'institution':\n",
    "            # Institution and country focused\n",
    "            text = \"\"\n",
    "            if affiliations_list and affiliations_list != 'None':\n",
    "                text += f\"Institutions: {affiliations_list}. \"\n",
    "            if countries_list and countries_list != 'None':\n",
    "                text += f\"Countries: {countries_list}. \"\n",
    "            # Add title for context\n",
    "            if title:\n",
    "                text += f\"Title: {title}\"\n",
    "            # If no institution data, skip this article for institution index\n",
    "            if not text.strip() or text.strip() == f\"Title: {title}\":\n",
    "                continue\n",
    "                \n",
    "        elif index_type == 'full':\n",
    "            # Everything combined\n",
    "            text = \"\"\n",
    "            if title:\n",
    "                text += f\"{title}. \"\n",
    "            if abstract:\n",
    "                text += f\"{abstract}. \"\n",
    "            if keywords:\n",
    "                text += f\"Keywords: {keywords}. \"\n",
    "            if authors_list and authors_list != 'None':\n",
    "                text += f\"Authors: {authors_list}. \"\n",
    "            if affiliations_list and affiliations_list != 'None':\n",
    "                text += f\"Institutions: {affiliations_list}. \"\n",
    "            if countries_list and countries_list != 'None':\n",
    "                text += f\"Countries: {countries_list}. \"\n",
    "        \n",
    "        text = text.strip()\n",
    "        if text:  # Only add if we have text\n",
    "            texts_to_embed.append(text)\n",
    "            article_ids.append(scopus_id)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(texts_to_embed):,} text entries for {index_type} index\")\n",
    "    return texts_to_embed, article_ids\n",
    "\n",
    "if DATABASE_PATH:\n",
    "    # Connect to database and explore\n",
    "    conn = sqlite3.connect(DATABASE_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check table structure\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(\"Available tables:\", [table[0] for table in tables])\n",
    "    \n",
    "    # Count articles with abstracts\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM articles WHERE abstract IS NOT NULL AND abstract != ''\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"\\nArticles with abstracts: {count:,}\")\n",
    "    \n",
    "    # Sample data\n",
    "    cursor.execute(\"SELECT title, abstract FROM articles WHERE abstract IS NOT NULL LIMIT 3\")\n",
    "    samples = cursor.fetchall()\n",
    "    \n",
    "    print(\"\\nüìÑ Sample articles:\")\n",
    "    for i, (title, abstract) in enumerate(samples, 1):\n",
    "        print(f\"\\n{i}. {title[:100]}...\")\n",
    "        print(f\"   Abstract: {abstract[:200]}...\")\n",
    "    \n",
    "    # Example usage of the embedding function\n",
    "    cursor.execute(\"SELECT * FROM articles WHERE abstract IS NOT NULL\")\n",
    "    articles_data = cursor.fetchall()\n",
    "    \n",
    "    # Create embeddings for different index types\n",
    "    for index_type in ['content', 'metadata', 'institution', 'full']:\n",
    "        create_embeddings_for_index_type(articles_data, index_type)\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"Cannot proceed without database file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79683596",
   "metadata": {},
   "source": [
    "## 4. Load SPECTER Model\n",
    "\n",
    "SPECTER is specifically trained on scientific papers and provides better embeddings for academic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17039cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß¨ Loading scientific text embedding model...\")\n",
    "\n",
    "# Load SPECTER model with fallbacks for scientific papers\n",
    "try:\n",
    "    # Primary: SPECTER - designed specifically for scientific papers\n",
    "    model = SentenceTransformer('allenai/specter')\n",
    "    print(\"‚úÖ SPECTER model loaded successfully!\")\n",
    "    model_name = \"SPECTER\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è SPECTER loading failed: {e}\")\n",
    "    try:\n",
    "        # Fallback 1: SciBERT - scientific domain BERT\n",
    "        model = SentenceTransformer('allenai/scibert_scivocab_uncased')\n",
    "        print(\"‚úÖ Using SciBERT as fallback\")\n",
    "        model_name = \"SciBERT\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SciBERT loading failed: {e}\")\n",
    "        # Fallback 2: General purpose model\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"‚úÖ Using MiniLM as last resort\")\n",
    "        model_name = \"MiniLM\"\n",
    "\n",
    "print(f\"üìù Using model: {model_name}\")\n",
    "print(f\"üìê Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Check if model is on GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = model.device\n",
    "    print(f\"üîß Model device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c8853",
   "metadata": {},
   "source": [
    "## 5. Extract and Prepare Article Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATABASE_PATH and model:\n",
    "    print(\"üìö Extracting article data from database...\")\n",
    "    \n",
    "    conn = sqlite3.connect(DATABASE_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # First, let's check what date range we actually have\n",
    "    print(\"üîç Analyzing date distribution in database...\")\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            substr(cover_date, 1, 4) as year,\n",
    "            COUNT(*) as count\n",
    "        FROM articles \n",
    "        WHERE cover_date IS NOT NULL AND cover_date != ''\n",
    "        AND abstract IS NOT NULL AND abstract != ''\n",
    "        GROUP BY substr(cover_date, 1, 4)\n",
    "        ORDER BY year\n",
    "    ''')\n",
    "    \n",
    "    year_distribution = cursor.fetchall()\n",
    "    print(\"üìÖ Articles by year:\")\n",
    "    for year, count in year_distribution:\n",
    "        print(f\"   {year}: {count:,} articles\")\n",
    "    \n",
    "    # Extract articles with enhanced metadata including affiliations and countries\n",
    "    print(\"\\nüìã Fetching articles with complete metadata including affiliations...\")\n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            A.scopus_id, \n",
    "            A.title, \n",
    "            A.abstract, \n",
    "            A.cover_date, \n",
    "            A.keywords,\n",
    "            GROUP_CONCAT(Auth.full_name, '; ') AS authors_list,\n",
    "            GROUP_CONCAT(Aff.institution_name, '; ') AS affiliations_list,\n",
    "            GROUP_CONCAT(Aff.country, '; ') AS countries_list\n",
    "        FROM articles AS A\n",
    "        LEFT JOIN article_authors AS AA ON A.scopus_id = AA.article_scopus_id\n",
    "        LEFT JOIN authors AS Auth ON AA.author_id = Auth.author_id\n",
    "        LEFT JOIN author_affiliations AS AuthAff ON Auth.author_id = AuthAff.author_id\n",
    "        LEFT JOIN affiliations AS Aff ON AuthAff.affiliation_id = Aff.affiliation_id\n",
    "        WHERE A.abstract IS NOT NULL AND A.abstract != '' \n",
    "        GROUP BY A.scopus_id, A.title, A.abstract, A.cover_date, A.keywords\n",
    "        ORDER BY A.scopus_id\n",
    "    ''')\n",
    "    \n",
    "    articles_data = cursor.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(articles_data):,} articles with complete metadata\")\n",
    "    \n",
    "    if not articles_data:\n",
    "        print(\"‚ùå No articles with abstracts found in the database.\")\n",
    "    else:\n",
    "        # Check the structure of the data\n",
    "        print(f\"üìä Data structure: {len(articles_data[0])} fields per article\")\n",
    "        sample = articles_data[0]\n",
    "        print(f\"üìÑ Sample data fields:\")\n",
    "        print(f\"   1. Scopus ID: {sample[0]}\")\n",
    "        print(f\"   2. Title: {sample[1][:50]}...\")\n",
    "        print(f\"   3. Abstract: {sample[2][:50]}...\")\n",
    "        print(f\"   4. Cover Date: {sample[3]}\")\n",
    "        print(f\"   5. Keywords: {sample[4]}\")\n",
    "        print(f\"   6. Authors: {sample[5][:50] if sample[5] else 'None'}...\")\n",
    "        print(f\"   7. Affiliations: {sample[6][:50] if sample[6] else 'None'}...\")\n",
    "        print(f\"   8. Countries: {sample[7][:50] if sample[7] else 'None'}...\")\n",
    "        \n",
    "        # Show year distribution of fetched data\n",
    "        from collections import Counter\n",
    "        years = [row[3][:4] for row in articles_data if row[3]]\n",
    "        year_counts = Counter(years)\n",
    "        print(f\"\\nüìä Final dataset year distribution:\")\n",
    "        for year in sorted(year_counts.keys()):\n",
    "            print(f\"   {year}: {year_counts[year]:,} articles\")\n",
    "        \n",
    "        # Check for any 2025 articles that shouldn't be there\n",
    "        articles_2025 = [row for row in articles_data if row[3] and row[3].startswith('2025')]\n",
    "        if articles_2025:\n",
    "            print(f\"\\n‚ö†Ô∏è WARNING: Found {len(articles_2025)} articles from 2025!\")\n",
    "            print(\"Sample 2025 articles:\")\n",
    "            for i, article in enumerate(articles_2025[:3], 1):\n",
    "                print(f\"   {i}. ID: {article[0]}, Date: {article[3]}, Title: {article[1][:50]}...\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Good: No 2025 articles found in database\")\n",
    "        \n",
    "        # Prepare simplified data for basic embedding (title + abstract)\n",
    "        article_ids = []\n",
    "        texts_to_embed = []\n",
    "        \n",
    "        print(\"\\nüîß Preparing texts for basic embedding...\")\n",
    "        for row in articles_data:\n",
    "            scopus_id, title, abstract, cover_date, keywords, authors_list, affiliations_list, countries_list = row\n",
    "            \n",
    "            # Optimize format for SPECTER: title + abstract\n",
    "            combined_text = \"\"\n",
    "            if title:\n",
    "                combined_text += f\"{title}. \"\n",
    "            if abstract:\n",
    "                combined_text += f\"{abstract}\"\n",
    "            \n",
    "            article_ids.append(scopus_id)\n",
    "            texts_to_embed.append(combined_text.strip())\n",
    "        \n",
    "        print(f\"‚úÖ Prepared {len(texts_to_embed):,} texts for embedding\")\n",
    "        print(f\"üìÑ Average text length: {np.mean([len(text) for text in texts_to_embed]):.0f} characters\")\n",
    "        \n",
    "        def build_faiss_index(texts, model, index_type):\n",
    "            \"\"\"Build and return a FAISS index with proper normalization.\"\"\"\n",
    "            print(f\"üî• Generating embeddings for {len(texts):,} texts ({index_type})...\")\n",
    "            \n",
    "            # Generate embeddings in batches for memory efficiency\n",
    "            embeddings = model.encode(texts, \n",
    "                                     batch_size=8,\n",
    "                                     show_progress_bar=True,\n",
    "                                     convert_to_numpy=True,\n",
    "                                     normalize_embeddings=False)  # We'll normalize manually\n",
    "            \n",
    "            embeddings = embeddings.astype('float32')\n",
    "            print(f\"üìê Embeddings shape: {embeddings.shape}\")\n",
    "            \n",
    "            # Build FAISS index with Inner Product for cosine similarity\n",
    "            dimension = embeddings.shape[1]\n",
    "            index = faiss.IndexFlatIP(dimension)  # Inner Product for cosine similarity\n",
    "            \n",
    "            # Normalize embeddings for cosine similarity\n",
    "            faiss.normalize_L2(embeddings)\n",
    "            print(\"üîß Normalized embeddings for cosine similarity\")\n",
    "            \n",
    "            # Add embeddings to index\n",
    "            index.add(embeddings)\n",
    "            print(f\"‚úÖ FAISS index built: {index.ntotal:,} vectors, {dimension} dimensions\")\n",
    "            \n",
    "            return index, embeddings\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without database and model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c170651",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings\n",
    "\n",
    "This is the most computationally intensive step. We'll process articles in batches for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'texts_to_embed' in locals() and len(texts_to_embed) > 0:\n",
    "    print(f\"üöÄ Generating {model_name} embeddings for {len(texts_to_embed):,} articles...\")\n",
    "    print(\"This may take several minutes depending on the number of articles and hardware.\")\n",
    "    \n",
    "    # Determine batch size based on available memory and model\n",
    "    if model_name == \"SPECTER\":\n",
    "        batch_size = 8 if torch.cuda.is_available() else 4\n",
    "    else:\n",
    "        batch_size = 32 if torch.cuda.is_available() else 16\n",
    "    \n",
    "    print(f\"üì¶ Using batch size: {batch_size}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(\n",
    "        texts_to_embed, \n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=model.device\n",
    "    )\n",
    "    \n",
    "    # Convert to float32 for FAISS compatibility\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    print(f\"‚úÖ Generated embeddings!\")\n",
    "    print(f\"üìè Embedding shape: {embeddings.shape}\")\n",
    "    print(f\"üíæ Memory usage: {embeddings.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Clear some memory\n",
    "    del texts_to_embed\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚ùå No texts to embed.\")\n",
    "\n",
    "# üöÄ Enhanced Multi-Index Creation Process\n",
    "print(\"üöÄ Starting Enhanced Semantic Indexing Process...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create each specialized index\n",
    "for index_type, config in INDEXES.items():\n",
    "    print(f\"\\nüîç Creating {index_type} index: {config['description']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Prepare texts for this index type\n",
    "    texts, article_ids = create_embeddings_for_index_type(articles_data, index_type)\n",
    "    \n",
    "    if not texts:\n",
    "        print(f\"‚ö†Ô∏è No texts found for {index_type} index\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"üìÑ Processing {len(texts):,} texts for {index_type} index\")\n",
    "    \n",
    "    # Build FAISS index\n",
    "    index, embeddings = build_faiss_index(texts, model, index_type)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, config['faiss_file'])\n",
    "    print(f\"üíæ Saved FAISS index: {config['faiss_file']}\")\n",
    "    \n",
    "    # Save article IDs mapping\n",
    "    with open(config['ids_file'], 'w') as f:\n",
    "        json.dump(article_ids, f)\n",
    "    print(f\"üíæ Saved article IDs: {config['ids_file']}\")\n",
    "    \n",
    "    # Check file sizes\n",
    "    faiss_size = os.path.getsize(config['faiss_file']) / (1024*1024)\n",
    "    ids_size = os.path.getsize(config['ids_file']) / (1024*1024)\n",
    "    print(f\"üìä Files created: {config['faiss_file']} ({faiss_size:.1f} MB), {config['ids_file']} ({ids_size:.2f} MB)\")\n",
    "    \n",
    "    print(f\"‚úÖ {index_type} index complete: {len(article_ids):,} articles, {embeddings.shape[1]} dimensions\")\n",
    "\n",
    "print(\"\\nüéâ Enhanced semantic indexing complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ceff8c",
   "metadata": {},
   "source": [
    "## 7. Build FAISS Index\n",
    "\n",
    "Create a FAISS index optimized for cosine similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa92999",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'embeddings' in locals():\n",
    "    print(\"üèóÔ∏è Building FAISS index...\")\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    print(f\"üìè Vector dimension: {dimension}\")\n",
    "    \n",
    "    # Use Inner Product index for cosine similarity (recommended for SPECTER)\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    # Normalize embeddings for proper cosine similarity\n",
    "    print(\"üîß Normalizing embeddings for cosine similarity...\")\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    print(\"üì• Adding embeddings to FAISS index...\")\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"‚úÖ FAISS index built successfully!\")\n",
    "    print(f\"üìä Index contains {index.ntotal:,} vectors\")\n",
    "    print(f\"üéØ Index type: {type(index).__name__} (Inner Product for cosine similarity)\")\n",
    "    \n",
    "    # Test the index with a sample query\n",
    "    print(\"\\nüß™ Testing index with sample query...\")\n",
    "    test_query = \"machine learning artificial intelligence\"\n",
    "    test_embedding = model.encode([test_query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(test_embedding)\n",
    "    \n",
    "    similarities, indices = index.search(test_embedding, 3)\n",
    "    print(f\"Sample query: '{test_query}'\")\n",
    "    print(f\"Top 3 results: indices {indices[0]}, similarities {similarities[0]}\")\n",
    "    \n",
    "    # üîç Validate Created Indexes\n",
    "    print(\"üîç Validating created indexes...\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    total_indexes_created = 0\n",
    "    total_size_mb = 0\n",
    "\n",
    "    for index_type, config in INDEXES.items():\n",
    "        faiss_file = config['faiss_file']\n",
    "        ids_file = config['ids_file']\n",
    "        \n",
    "        if os.path.exists(faiss_file) and os.path.exists(ids_file):\n",
    "            # Load and test index\n",
    "            test_index = faiss.read_index(faiss_file)\n",
    "            with open(ids_file, 'r') as f:\n",
    "                test_ids = json.load(f)\n",
    "            \n",
    "            faiss_size = os.path.getsize(faiss_file) / (1024*1024)\n",
    "            ids_size = os.path.getsize(ids_file) / (1024*1024)\n",
    "            total_size_mb += faiss_size + ids_size\n",
    "            \n",
    "            print(f\"‚úÖ {index_type}: {test_index.ntotal:,} vectors, {len(test_ids):,} IDs ({faiss_size:.1f} MB)\")\n",
    "            total_indexes_created += 1\n",
    "            \n",
    "            # Quick test search\n",
    "            if test_index.ntotal > 0:\n",
    "                test_query = model.encode([\"machine learning artificial intelligence\"])\n",
    "                test_query = test_query.astype('float32')\n",
    "                faiss.normalize_L2(test_query)\n",
    "                distances, indices = test_index.search(test_query, 3)\n",
    "                print(f\"   Test search successful: top similarity = {1-distances[0][0]:.3f}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {index_type}: Files missing\")\n",
    "\n",
    "    print(f\"\\nüìä Summary: {total_indexes_created}/{len(INDEXES)} indexes created successfully\")\n",
    "    print(f\"üíæ Total size: {total_size_mb:.1f} MB\")\n",
    "    print(f\"üéØ Ready for deployment to Hugging Face Spaces!\")\n",
    "else:\n",
    "    print(\"‚ùå No embeddings available to build index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6fdf1",
   "metadata": {},
   "source": [
    "## 8. Save Index and Metadata\n",
    "\n",
    "Save the FAISS index and article ID mapping for use in the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a819bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'index' in locals() and 'article_ids' in locals():\n",
    "    print(\"üíæ Saving FAISS index and metadata...\")\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, FAISS_INDEX_FILE)\n",
    "    print(f\"‚úÖ FAISS index saved: {FAISS_INDEX_FILE}\")\n",
    "    \n",
    "    # Save article IDs mapping\n",
    "    with open(ARTICLE_IDS_MAP_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article_ids, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Article IDs saved: {ARTICLE_IDS_MAP_FILE}\")\n",
    "    \n",
    "    # File sizes\n",
    "    index_size = os.path.getsize(FAISS_INDEX_FILE) / 1024**2\n",
    "    ids_size = os.path.getsize(ARTICLE_IDS_MAP_FILE) / 1024**2\n",
    "    \n",
    "    print(f\"\\nüìÅ File sizes:\")\n",
    "    print(f\"   - FAISS index: {index_size:.1f} MB\")\n",
    "    print(f\"   - Article IDs: {ids_size:.1f} MB\")\n",
    "    print(f\"   - Total: {index_size + ids_size:.1f} MB\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüéâ Semantic indexing completed successfully!\")\n",
    "    print(f\"üìä Statistics:\")\n",
    "    print(f\"   - Model used: {model_name}\")\n",
    "    print(f\"   - Articles indexed: {len(article_ids):,}\")\n",
    "    print(f\"   - Vector dimension: {dimension}\")\n",
    "    print(f\"   - Index type: Inner Product (cosine similarity)\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot save files - index or article_ids not available.\")\n",
    "\n",
    "# üîÑ Create Compatibility with Existing Single Index System\n",
    "print(\"üîÑ Creating compatibility with existing system...\")\n",
    "\n",
    "# The main chatbot (app_intelligent.py) currently uses these files:\n",
    "MAIN_FAISS_FILE = \"scopus_combined_metadata_index.faiss\"\n",
    "MAIN_IDS_FILE = \"scopus_article_ids_for_index.json\"\n",
    "\n",
    "# Use the 'metadata' index as the main index (best balance of content and metadata)\n",
    "if os.path.exists(INDEXES['metadata']['faiss_file']) and os.path.exists(INDEXES['metadata']['ids_file']):\n",
    "    \n",
    "    # Copy metadata index as main index for compatibility\n",
    "    import shutil\n",
    "    \n",
    "    shutil.copy2(INDEXES['metadata']['faiss_file'], MAIN_FAISS_FILE)\n",
    "    shutil.copy2(INDEXES['metadata']['ids_file'], MAIN_IDS_FILE)\n",
    "    \n",
    "    # Verify the copy\n",
    "    main_index = faiss.read_index(MAIN_FAISS_FILE)\n",
    "    with open(MAIN_IDS_FILE, 'r') as f:\n",
    "        main_ids = json.load(f)\n",
    "    \n",
    "    main_size = os.path.getsize(MAIN_FAISS_FILE) / (1024*1024)\n",
    "    \n",
    "    print(f\"‚úÖ Main index created: {MAIN_FAISS_FILE}\")\n",
    "    print(f\"üìä {main_index.ntotal:,} vectors, {len(main_ids):,} article IDs ({main_size:.1f} MB)\")\n",
    "    print(f\"üéØ Compatible with existing app_intelligent.py chatbot\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Metadata index not found - cannot create main compatibility index\")\n",
    "\n",
    "print(\"\\nüéâ Enhanced semantic indexing system ready!\")\n",
    "print(\"üìÅ Created files can be uploaded to Hugging Face Spaces for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe31e7b",
   "metadata": {},
   "source": [
    "## 9. Verification and Testing\n",
    "\n",
    "Verify that the saved files can be loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af00f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved files\n",
    "if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(ARTICLE_IDS_MAP_FILE):\n",
    "    print(\"üîç Verifying saved files...\")\n",
    "    \n",
    "    # Load and test FAISS index\n",
    "    try:\n",
    "        test_index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        print(f\"‚úÖ FAISS index loaded successfully: {test_index.ntotal:,} vectors\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading FAISS index: {e}\")\n",
    "    \n",
    "    # Load and test article IDs\n",
    "    try:\n",
    "        with open(ARTICLE_IDS_MAP_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_article_ids = json.load(f)\n",
    "        print(f\"‚úÖ Article IDs loaded successfully: {len(test_article_ids):,} IDs\")\n",
    "        \n",
    "        # Show sample IDs\n",
    "        print(f\"üìÑ Sample article IDs: {test_article_ids[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading article IDs: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All files verified successfully!\")\n",
    "    print(\"\\nüì• Next steps:\")\n",
    "    print(\"1. Download both files from Kaggle output:\")\n",
    "    print(f\"   - {os.path.basename(FAISS_INDEX_FILE)}\")\n",
    "    print(f\"   - {os.path.basename(ARTICLE_IDS_MAP_FILE)}\")\n",
    "    print(\"2. Place them in your local Scopus ChatBot directory\")\n",
    "    print(\"3. Run your chatbot application!\")\n",
    "else:\n",
    "    print(\"‚ùå Output files not found. Check the previous steps for errors.\")\n",
    "\n",
    "# üß™ Advanced Search Testing\n",
    "print(\"üß™ Testing different search strategies...\")\n",
    "\n",
    "def test_search_strategy(query, index_type, top_k=5):\n",
    "    \"\"\"Test search on a specific index type.\"\"\"\n",
    "    config = INDEXES[index_type]\n",
    "    \n",
    "    if not os.path.exists(config['faiss_file']):\n",
    "        print(f\"‚ùå {index_type} index not found\")\n",
    "        return\n",
    "    \n",
    "    # Load index\n",
    "    index = faiss.read_index(config['faiss_file'])\n",
    "    with open(config['ids_file'], 'r') as f:\n",
    "        article_ids = json.load(f)\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query])\n",
    "    query_embedding = query_embedding.astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    print(f\"\\nüîç Query: '{query}' on {index_type} index\")\n",
    "    print(f\"üìä {config['description']}\")\n",
    "    \n",
    "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(article_ids):\n",
    "            similarity = 1 - distance\n",
    "            print(f\"  {i+1}. ID: {article_ids[idx][:20]}... | Similarity: {similarity:.3f}\")\n",
    "\n",
    "# Test different types of queries\n",
    "test_queries = [\n",
    "    (\"machine learning artificial intelligence\", \"content\"),\n",
    "    (\"machine learning neural networks\", \"metadata\"), \n",
    "    (\"research from China university\", \"institution\"),\n",
    "    (\"COVID-19 treatment drug discovery\", \"full\")\n",
    "]\n",
    "\n",
    "for query, best_index in test_queries:\n",
    "    test_search_strategy(query, best_index, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9235f",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis\n",
    "\n",
    "Analyze the performance and quality of the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e598ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'embeddings' in locals() and 'model' in locals():\n",
    "    print(\"üìä Performance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test with sample scientific queries\n",
    "    test_queries = [\n",
    "        \"machine learning applications in healthcare\",\n",
    "        \"deep neural networks for image recognition\",\n",
    "        \"COVID-19 vaccine development\",\n",
    "        \"artificial intelligence natural language processing\",\n",
    "        \"climate change environmental impact\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üß™ Testing with {len(test_queries)} sample queries...\\n\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        # Generate query embedding\n",
    "        query_emb = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        \n",
    "        # Search top 3 results\n",
    "        similarities, indices = index.search(query_emb, 3)\n",
    "        \n",
    "        print(f\"{i}. Query: '{query}'\")\n",
    "        # Fix: Access individual elements properly\n",
    "        sim_scores = similarities[0]\n",
    "        print(f\"   Top similarities: {sim_scores[0]:.3f}, {sim_scores[1]:.3f}, {sim_scores[2]:.3f}\")\n",
    "        \n",
    "        # Show article IDs of top results\n",
    "        top_article_ids = [article_ids[idx] for idx in indices[0]]\n",
    "        print(f\"   Top article IDs: {top_article_ids}\")\n",
    "        print()\n",
    "    \n",
    "    # Embedding statistics\n",
    "    print(\"üìà Embedding Statistics:\")\n",
    "    print(f\"   - Total vectors: {embeddings.shape[0]:,}\")\n",
    "    print(f\"   - Dimensions: {embeddings.shape[1]}\")\n",
    "    print(f\"   - Mean norm: {np.mean(np.linalg.norm(embeddings, axis=1)):.3f}\")\n",
    "    print(f\"   - Std norm: {np.std(np.linalg.norm(embeddings, axis=1)):.3f}\")\n",
    "    \n",
    "    print(\"\\nüéØ The semantic index is ready for production use!\")\n",
    "    \n",
    "    # üìã Final Summary and Deployment Guide\n",
    "\n",
    "    print(\"üìã ENHANCED SEMANTIC INDEXING COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # List all created files\n",
    "    created_files = []\n",
    "    total_size = 0\n",
    "\n",
    "    print(\"üìÅ Created Index Files:\")\n",
    "    for index_type, config in INDEXES.items():\n",
    "        if os.path.exists(config['faiss_file']) and os.path.exists(config['ids_file']):\n",
    "            faiss_size = os.path.getsize(config['faiss_file']) / (1024*1024)\n",
    "            ids_size = os.path.getsize(config['ids_file']) / (1024*1024)\n",
    "            total_size += faiss_size + ids_size\n",
    "            \n",
    "            print(f\"  ‚úÖ {index_type}:\")\n",
    "            print(f\"     ‚Ä¢ {config['faiss_file']} ({faiss_size:.1f} MB)\")\n",
    "            print(f\"     ‚Ä¢ {config['ids_file']} ({ids_size:.2f} MB)\")\n",
    "            \n",
    "            created_files.extend([config['faiss_file'], config['ids_file']])\n",
    "\n",
    "    # Main compatibility files\n",
    "    if os.path.exists(MAIN_FAISS_FILE):\n",
    "        main_size = os.path.getsize(MAIN_FAISS_FILE) / (1024*1024)\n",
    "        total_size += main_size\n",
    "        print(f\"  ‚úÖ Main Compatibility:\")\n",
    "        print(f\"     ‚Ä¢ {MAIN_FAISS_FILE} ({main_size:.1f} MB)\")\n",
    "        print(f\"     ‚Ä¢ {MAIN_IDS_FILE}\")\n",
    "        created_files.extend([MAIN_FAISS_FILE, MAIN_IDS_FILE])\n",
    "\n",
    "    print(f\"\\nüíæ Total Files: {len(created_files)} | Total Size: {total_size:.1f} MB\")\n",
    "\n",
    "    print(f\"\\nüöÄ DEPLOYMENT READY!\")\n",
    "    print(\"üì§ Upload these files to Hugging Face Spaces along with:\")\n",
    "    print(\"   ‚Ä¢ app_intelligent.py (main chatbot)\")\n",
    "    print(\"   ‚Ä¢ scopus_database.db (database)\")\n",
    "    print(\"   ‚Ä¢ requirements.txt (dependencies)\")\n",
    "\n",
    "    print(f\"\\nüéØ Your chatbot supports:\")\n",
    "    print(\"   ‚úÖ Natural language queries: 'machine learning papers from 2023'\")\n",
    "    print(\"   ‚úÖ Author searches: 'research by Smith'\")\n",
    "    print(\"   ‚úÖ Geographic searches: 'articles from China'\")\n",
    "    print(\"   ‚úÖ Institution searches: 'Harvard research on AI'\")\n",
    "    print(\"   ‚úÖ Semantic searches: 'deep learning neural networks'\")\n",
    "\n",
    "    print(f\"\\nüìä Dataset: 4,000+ scientific articles from Scopus (2018-2025)\")\n",
    "    print(f\"üß† Technology: FAISS + {model_name} embeddings + Intelligent query parsing\")\n",
    "    print(f\"üéâ Ready for production deployment!\")\n",
    "\n",
    "    # Save creation log\n",
    "    with open('indexing_log.txt', 'w') as f:\n",
    "        f.write(f\"Enhanced Semantic Indexing Completed\\n\")\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Articles processed: {len(articles_data):,}\\n\")\n",
    "        f.write(f\"Indexes created: {len(created_files)//2}\\n\")\n",
    "        f.write(f\"Total size: {total_size:.1f} MB\\n\")\n",
    "        f.write(f\"Files: {', '.join(created_files)}\\n\")\n",
    "\n",
    "    print(\"üìù Indexing log saved to 'indexing_log.txt'\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform analysis - embeddings not available.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
